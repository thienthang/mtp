{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 - Lab - Logistic Regression\n",
    "\n",
    "### Sentiment analysis\n",
    "\n",
    "This contest is taken from the real task of Text Processing.\n",
    "\n",
    "The task is to build a model that will determine the tone (positive, negative) of the text. To do this, you will need to train the model on the existing data (train.csv). The resulting model will have to determine the class (neutral, positive, negative) of new texts. The dataset contains the following fields:\n",
    "\n",
    "| Field name | Meaning |\n",
    "|------------|-----------|\n",
    "| ItemID  | id of twit|\n",
    "| Sentiment | sentiment (1-positive, 0-negative)|\n",
    "| SentimentText | text of the twit|\n",
    "\n",
    "Let's first of all have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ...\n",
       "5       6          0                  or i just worry too much?        \n",
       "6       7          1                 Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
       "7       8          0         Sunny Again        Work Tomorrow  :-|  ...\n",
       "8       9          1        handed in my uniform today . i miss you ...\n",
       "9      10          1           hmmmm.... i wonder how she my number @-)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas, numpy and the dataset, save it in a object called 'sentiment'\n",
    "# Your code here\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "sentiment = pd.read_csv('data/train.csv', encoding='latin-1')\n",
    "# Let's check sentiment.head(10) and sample(10)\n",
    "# Your code here\n",
    "sentiment.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the structure of a twit varies a lot between twit and twit. They have different lengths, letters, numbers, extrange characters, etc. \n",
    "\n",
    "It is also important to note that **a lot** of words are not correctly spelled, for example the word _\"Juuuuuuuuuuuuuuuuussssst\"_ or the word _\"sooo\"_\n",
    "\n",
    "This makes it hard to mesure how positive or negative are the words within the twits.\n",
    "\n",
    "So we need a way of scoring the words such that words that appear in positive twits have greater score that those that appear in negative twits.\n",
    "\n",
    "But first... how do we represent the twits as vectors we can input to our algorithm?\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "One thing we could do to represent the twits as equal-sized vectors of numbers is the following:\n",
    "\n",
    "* Create a list (vocabulary) with all the unique words in the whole corpus of twits. \n",
    "* We construct a feature vector from each twit that contains the counts of how often each word occurs in the particular twit\n",
    "\n",
    "_Note that since the unique words in each twit represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros_\n",
    "\n",
    "Lets construct the bag of words. We will work with a smaller example for illustrative purposes, and at the end we will work with our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "twits = [\n",
    "    'This is amazing!',\n",
    "    'ML is the best, yes it is',\n",
    "    'I am not sure about how this is going to end...'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import [CountVectorizer.](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) It'll help us to convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'am', 'amazing', 'best', 'end', 'going', 'how', 'is', 'it', 'ml', 'not', 'sure', 'the', 'this', 'to', 'yes']\n",
      "[[0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 2 1 1 0 0 1 0 0 1]\n",
      " [1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define an object of CountVectorizer() fit and transfom your twits into a 'bag'\n",
    "# Your code here\n",
    "vectorizer = CountVectorizer()\n",
    "bag = vectorizer.fit_transform(twits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'am', 'amazing', 'best', 'end', 'going', 'how', 'is', 'it', 'ml', 'not', 'sure', 'the', 'this', 'to', 'yes']\n"
     ]
    }
   ],
   "source": [
    "# Find in document of CountVectorizer a function that show us list of feature names\n",
    "# Your code here\n",
    "ftnameList = vectorizer.get_feature_names()\n",
    "print(ftnameList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from executing the preceding command, the vocabulary is stored in a Python array that maps the unique words to integer indices. Next, let's print the feature vectors that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 2 1 1 0 0 1 0 0 1]\n",
      " [1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Call toarray() on your 'bag' to see the feature vectors\n",
    "# Your code here\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the index of the word 'is' and how many times it occurs in all three twits?\n",
    "# Your answer here\n",
    "# vectorizer.vocabulary_.get('is')\n",
    "index = vectorizer.get_feature_names().index('is')\n",
    "bag.toarray()[:, index].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each index position in the feature vectors corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the first feature at index position 0 resembles the count of the word 'about' , which only occurs in the last document. These values in the feature vectors are also called the **raw term frequencies**: `tf(t,d )` —the number of times a term `t` occurs in a document `d`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How relevant are words? Term frequency-inverse document frequency\n",
    "\n",
    "We could use these raw term frequencies to score the words in our algorithm. There is a problem though: If a word is very frequent in _all_ documents, then it probably doesn't carry a lot of information. In order to tacke this problem we can use **term frequency-inverse document frequency**, which will reduce the score the more frequent the word is accross all twits. It is calculated like this:\n",
    "\n",
    "\\begin{equation*}\n",
    "tf-idf(t,d) = tf(t,d) ~ idf(t,d)\n",
    "\\end{equation*}\n",
    "\n",
    "_tf(t,d)_ is the raw term frequency descrived above. _idf(t,d)_ is the inverse document frequency, than can be calculated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log \\frac{n_d}{1+df\\left(d,t\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "where `n` is the total number of documents and _df(t,d)_ is the number of documents where the term `t` appears. \n",
    "\n",
    "The `1` addition in the denominator is just to avoid zero term for terms that appear in all documents, will not be entirely ignored. Ans the `log` ensures that low frequency term don't get too much weight.\n",
    "\n",
    "Fortunately for us `scikit-learn` does all those calculations for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# Formatting the number to 2 digits after the decimal point by showing on this notebook\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Feed the tf-idf transformer with our previously created Bag of Words using fit_transform()\n",
    "# Your code here\n",
    "bag2 = tfidf.fit_transform(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.72, 0.  , 0.  , 0.  , 0.  , 0.43, 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.55, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.4 , 0.  , 0.  , 0.  , 0.47, 0.4 , 0.4 , 0.  ,\n",
       "        0.  , 0.4 , 0.  , 0.  , 0.4 ],\n",
       "       [0.33, 0.33, 0.  , 0.  , 0.33, 0.33, 0.33, 0.2 , 0.  , 0.  , 0.33,\n",
       "        0.33, 0.  , 0.25, 0.33, 0.  ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now what is the weight of the word 'is' and 'amazing'?\n",
    "# Your answer here\n",
    "bag2.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String manipulation in Python\n",
    "\n",
    "One place where the Python language really shines is in the manipulation of strings. This section will cover some of Python's built-in string methods and formatting operations, before moving on to a quick guide to the extremely useful subject of regular expressions. Such string manipulation patterns come up often in the context of data science work\n",
    "\n",
    "### Formatting strings: Adjusting case\n",
    "\n",
    "Python makes it quite easy to adjust the case of a string. Here we'll look at the `upper()`, `lower()`, `capitalize()`, and `swapcase()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox.\n",
      "The quick brown fox.\n",
      "ThE QuicK BrowN FoX.\n"
     ]
    }
   ],
   "source": [
    "fox = 'tHe qUICk bROWn fOx.'\n",
    "# Apply the functions above to `fox` and print out the results\n",
    "# Your code here\n",
    "print(fox.lower())\n",
    "print(fox.capitalize())\n",
    "print(fox.swapcase())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and removing spaces\n",
    "\n",
    "Another common need is to remove spaces (or other characters) from the beginning or end of the string. The basic method of removing characters is the `strip()` method, which strips whitespace from the beginning and end of the line. To remove just space to the right or left, use `rstrip()` or `lstrip()` respectively.\n",
    "\n",
    "To remove characters other than spaces, you can pass the desired character to the `strip()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         this is the content\n",
      "this is the content\n",
      "this is the content         \n",
      "435\n"
     ]
    }
   ],
   "source": [
    "line = '         this is the content         '\n",
    "\n",
    "# Apply strip(), rstrip(), lstrip() to 'line' and print the results out\n",
    "# Your code here\n",
    "print(line.rstrip())\n",
    "print(line.strip())\n",
    "print(line.lstrip())\n",
    "\n",
    "num = '00000000435'\n",
    "# Remove all of the zeros from num\n",
    "# Your code here\n",
    "print(num.replace('0',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding and replacing and splitting\n",
    "\n",
    "If you want to find occurrences of a certain character in a string, the `find()`, `index()` and `replace()` methods are the best built-in methods.\n",
    "\n",
    "`find()` and `index()` are very similar, in that they search for the first occurrence of a character or substring within a string, and return the index of the substring.\n",
    "\n",
    "The `split()` method is perhaps more useful; it finds all instances of the split-point and returns the substrings in between. The default is to split on any whitespace, returning a list of the individual words in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 'the quick brown fox jumped over a lazy dog'\n",
    "\n",
    "# Find the index of 'fox' in 'line' using find() and index()\n",
    "# Your code here\n",
    "\n",
    "# Let's replace 'brown' with 'red'\n",
    "# Your code here\n",
    "\n",
    "# List all words in 'line' and put them in an array\n",
    "# Your 1 line of code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you would like to undo a `split()`, you can use the `join()` method, which returns a string built from a splitpoint and an iterable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1--2--3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'--'.join(['1', '2', '3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common pattern is to use the special character \"\\n\" (newline) to join together lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules in family:\n",
      "1. Your wife is always right.\n",
      "2. If she is wrong, check the first rule again.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(['Rules in family:', '1. Your wife is always right.', '2. If she is wrong, check the first rule again.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression\n",
    "\n",
    "In Python, regular expressions are supported by the `re` module.\n",
    "\n",
    "A regular expression (or RE) specifies a set of strings that matches it; the functions in this module let you check if a particular string matches a given regular expression (or if a given regular expression matches a particular string, which comes down to the same thing).\n",
    "\n",
    "Let's walk through some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minhdh@coderschool.vn', 'haiminh101@yahoo.vn']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_regex = '\\w+@\\w+\\.[a-z]{2}'\n",
    "text = \"To email Hai Minh, try minhdh@coderschool.vn or the older address haiminh101@yahoo.vn\"\n",
    "re.findall(email_regex, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To email Hai Minh, try --@--.-- or the older address --@--.--'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing these email addresses with another string, perhaps to hide addresses in the output:\n",
    "re.sub(email_regex, '--@--.--', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'ns', 'q', '', 'nt', '', 'l']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following will match any lower-case vowel:\n",
    "re.split('[aeiou]', 'consequential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to extract from a document specific numerical codes that consist of a capital letter followed by a digit. You could do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G2', 'H6']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][0-9]', '1043879, G2, H6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table lists a few of these characters that are commonly useful:\n",
    "\n",
    "| Character | Description | Character | Description |\n",
    "|------------|-----------|------------|-----------|\n",
    "| \"\\d\" | Match any digit   | \"\\D\" | Match any non-digit|\n",
    "| \"\\s\" | Match any whitespace   | \"\\S\" | Match any non-whitespace|\n",
    "| \"\\w\" | Match any alphanumeric char  | \"\\W\" | Match any non-alphanumeric char|\n",
    "\n",
    "| Character | Description | Example |\n",
    "|------------|-----------|------------|\n",
    "| ? | Match zero or one repetitions of preceding |  \"ab?\" matches \"a\" or \"ab\" |\n",
    "| * | Match zero or more repetitions of preceding | \"ab*\" matches \"a\", \"ab\", \"abb\", \"abbb\"... |\n",
    "| + | Match one or more repetitions of preceding |  \"ab+\" matches \"ab\", \"abb\", \"abbb\"... but not \"a\" |\n",
    "| {n} | Match n repetitions of preceding | \"ab{2}\" matches \"abb\" |\n",
    "| {m,n} | Match between m and n repetitions of preceding |  \"ab{2,3}\" matches \"abb\" or \"abbb\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Resources on Regular Expressions\n",
    "\n",
    "* [Python's re package Documentation](https://docs.python.org/3/library/re.html)\n",
    "* [Python's official regular expression HOWTO](https://docs.python.org/3/howto/regex.html)\n",
    "* [Mastering Regular Expressions (OReilly, 2006)](http://shop.oreilly.com/product/9780596528126.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clean up\n",
    "\n",
    "### Removing stop words\n",
    "\n",
    "Now that we know how to format and score our input. Let's look at our **real** vocabulary. Specifically, the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'blue': 3, 'red': 2, 'green': 1})\n",
      "[('blue', 3), ('red', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Example\n",
    "count = Counter()\n",
    "\n",
    "for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:\n",
    "    count[word] += 1\n",
    "print(count)\n",
    "print(count.most_common(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'about': 1, 'am': 1, 'amazing': 1, 'best': 1, 'end': 1, 'going': 1, 'how': 1, 'is': 1, 'it': 1, 'ml': 1, 'not': 1, 'sure': 1, 'the': 1, 'this': 1, 'to': 1, 'yes': 1})\n",
      "[('about', 1), ('am', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "# Let's apply the example above to count words in our SentimentText\n",
    "# Your code here\n",
    "for word in ftnameList:\n",
    "    vocab[word] += 1\n",
    "print(vocab)\n",
    "print(vocab.most_common(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most common words are meaningless in terms of sentiment: _I, to, the, and_... they don't give any information on positiveness or negativeness. They're basically **noise** that can most probably be eliminated. These kind of words are called _stop words_, and it is a common practice to remove them when doing text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ted\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazing', 1),\n",
       " ('best', 1),\n",
       " ('end', 1),\n",
       " ('going', 1),\n",
       " ('ml', 1),\n",
       " ('sure', 1),\n",
       " ('yes', 1)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "vocab_reduced = Counter()\n",
    "# Go through all of the items of vocab using vocab.items() and pick only words that are not in 'stop' \n",
    "# and save them in vocab_reduced\n",
    "# Your code here\n",
    "#Recheck with index for item[0]\n",
    "items = vocab.items()\n",
    "for item, count in items:\n",
    "    if item not in stop:\n",
    "        vocab_reduced[item] = count\n",
    "\n",
    "vocab_reduced.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better, only in the 20 most common words we already see words that make sense: good, love, really... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing special characters and \"trash\"\n",
    "\n",
    "If you look closer, you'll see that we're also taking into consideration punctuation signs ('-', ',', etc) and other html tags like `&amp`. We can definitely remove them for the sentiment analysis, but we will try to keep the emoticons, since those _do_ have a sentiment load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "omg \n",
      "omg yamada 483 okniaza334438  \n"
     ]
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    #\"\"\" Return a cleaned version of text\n",
    "    #\"\"\"\n",
    "    if text is '' and text.strip() is '':\n",
    "        return 'sad'\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create some random texts for testing the function preprocessor()\n",
    "print(preprocessor(''))\n",
    "print(preprocessor('Omg'))\n",
    "print(preprocessor('Omg #$# yamada #*#483 okniaza334438***'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready! There is another trick we can use to reduce our vocabulary and consolidate words. If you think about it, words like: love, loving, etc. _Could_ express the same positivity. If that was the case, we would be  having two words in our vocabulary when we could have only one: lov. This process of reducing a word to its root is called **stemming**.\n",
    "\n",
    "We also need a _tokenizer_ to break down our twits in individual words. We will implement two tokenizers, a regular one and one that does steaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'there,', 'I', 'am', 'loving', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n",
      "['Hi', 'there,', 'I', 'am', 'love', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# write a function called `tokenizer()` that split a text into list of words\n",
    "# Your code here\n",
    "def tokenizer(txt):\n",
    "    return txt.strip().split()\n",
    "\n",
    "\n",
    "# write a function named `tokenizer_porter()` that split a text into list of words and apply stemming technic\n",
    "# Hint: porter.stem(word)\n",
    "# Your code here\n",
    "def tokenizer_porter(txt):\n",
    "    \n",
    "    return [porter.stem(word) for word in txt.split()]\n",
    "\n",
    "\n",
    "# Testing\n",
    "print(tokenizer('Hi there, I am loving this, like with a lot of love'))\n",
    "print(tokenizer_porter('Hi there, I am loving this, like with a lot of love'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic Regression\n",
    "\n",
    "We are finally ready to train our algorythm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69992,)\n",
      "(29997,)\n"
     ]
    }
   ],
   "source": [
    "sentiment\n",
    "# split the dataset in train and test\n",
    "# Your code here\n",
    "final_X = sentiment['SentimentText']\n",
    "final_y = sentiment['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_X, final_y, test_size = 0.3, random_state = 101)# in this our main data is splitted into train and test\n",
    "\n",
    "# we can check their dimension\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function preproc...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)\n",
    "\n",
    "# A pipeline is what chains several steps together, once the initial exploration is done. \n",
    "# For example, some codes are meant to transform features — normalise numericals, or turn text into vectors, \n",
    "# or fill up missing data, they are transformers; other codes are meant to predict variables by fitting an algorithm,\n",
    "# they are estimators. Pipeline chains all these together which can then be applied to training data\n",
    "clf = Pipeline([('vect', tfidf),\n",
    "                ('clf', LogisticRegression(random_state=0))])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run some tests :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is really bad --> Negative, Positive = [0.96 0.04]\n",
      "I love this! --> Negative, Positive = [0.08 0.92]\n",
      ":) --> Negative, Positive = [0.38 0.62]\n"
     ]
    }
   ],
   "source": [
    "twits = [\n",
    "    \"This is really bad\",\n",
    "    \"I love this!\",\n",
    "    \":)\",\n",
    "]\n",
    "\n",
    "preds = clf.predict_proba(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would like to use the classifier in another place, or just not train it again and again everytime, we can save the model in a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "pickle.dump(clf, open(os.path.join('data', 'logisticRegression.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And you're done! I hope you liked this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
